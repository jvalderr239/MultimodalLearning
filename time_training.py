# -*- coding: utf-8 -*-
"""DataPreProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iKfrE4WArlkpHOJmVrVuGR8vjg4fZ5Hr
"""

import os.path
import pickle
import numpy as np
from copy import deepcopy
import random
import time
from tqdm import tqdm
from matplotlib import pyplot as plt

import torch
import torch.optim as optim
import torch.nn as nn
import torchvision
import torchvision.transforms as tf
import torchvision.models as models
from torch.utils.data import TensorDataset, DataLoader

import albumentations as A
import librosa
from albumentations.pytorch import ToTensorV2
from albumentations.core.transforms_interface import ImageOnlyTransform
from fastai.vision.all import TimeDistributed

datapath = "./drive/MyDrive/Multimodal/"

class SpecAugment(ImageOnlyTransform):
    """Shifting time axis"""
    def __init__(self, num_mask=5, freq_masking=0.6, time_masking=0.5, always_apply=False, p=0.7):
        super(SpecAugment, self).__init__(always_apply, p)
        
        self.num_mask = num_mask
        self.freq_masking = freq_masking
        self.time_masking = time_masking
    
    def apply(self, data, **params):
        melspec = data
        spec_aug = self.spec_augment(melspec,)
        
        return spec_aug
    
    # Source: https://www.kaggle.com/davids1992/specaugment-quick-implementation
    def spec_augment(self, 
                    spec: np.ndarray,):
        spec = spec.copy()
        value = spec.min()
        num_mask = random.randint(1, self.num_mask)
        for i in range(num_mask):
            all_freqs_num, all_frames_num  = spec.shape
            freq_percentage = random.uniform(0.0, self.freq_masking)

            num_freqs_to_mask = int(freq_percentage * all_freqs_num)
            f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)
            f0 = int(f0)
            spec[f0:f0 + num_freqs_to_mask, :] = value

            time_percentage = random.uniform(0.0, self.time_masking)

            num_frames_to_mask = int(time_percentage * all_frames_num)
            t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)
            t0 = int(t0)
            spec[:, t0:t0 + num_frames_to_mask] = value

        return spec

class SpectToImage(ImageOnlyTransform):

    def __init__(self, always_apply=True, p=0.5):
        super(SpectToImage, self).__init__(always_apply, p)
    def apply(self, data, **params):
        image = data
        delta = librosa.feature.delta(image)
        accelerate = librosa.feature.delta(image, order=2)
        image = np.stack([image, delta, accelerate], axis=-1)
        image = image.astype(np.float32) / 100.0
        assert image.shape[-1] == 3
        return image

train_transform = {'visual':A.Compose([A.Resize(227,227),
    A.RandomRotate90(),
    A.Flip(),
    A.OneOf([A.IAAAdditiveGaussianNoise(),
    A.GaussNoise(),], p=0.2),
    A.OneOf([
            A.MotionBlur(p=.2),
            A.MedianBlur(blur_limit=3, p=0.1),
            A.Blur(blur_limit=3, p=0.1),
        ], p=0.2),
    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),
    A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),
    A.HueSaturationValue(p=0.3),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(),
    ]),
    'audio':A.Compose([
        SpecAugment(), 
        SpectToImage(),
        A.Resize(227,227),
        #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
        ])
    }
    
val_transform = {'visual': A.Compose([A.Resize(227,227),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(),
    ]),
    'audio':A.Compose([
        SpectToImage(),
        A.Resize(227,227),
        #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
        ])
    }

def one_hot_ce_loss(outputs, targets):
    criterion = nn.CrossEntropyLoss()
    return criterion(outputs, targets)

def train_model(name,
              dataloaders=None,
              num_epochs=25,
              _lr = .001,
              _model=models.alexnet(pretrained=True),
              _path=datapath,
              is_loaded=False,
              ):
  """
  :param name: name of the model
  :param dataloaders: data to use for train and validation
  :param num_epochs: epochs for training
  :param model: pytorch model to be trained
  :param criterion: loss function
  :param _path: path to store data
  :param is_multimodal: train audio and visual together
  :return:
  """
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print(f"Running {device}")
  since = time.time()
  _model.classifier[6] = nn.Identity()
  model = nn.Sequential(
      TimeDistributed(_model),
      nn.Flatten(),
      nn.AvgPool1d(3),
      nn.Linear((6 * 4096)//3, 8),
      nn.Softmax(dim=1)
      )
  if is_loaded:
      model.load_state_dict(torch.load(datapath + "/Models/" + model_name + "_model.pth"))
  if torch.cuda.is_available(): 
      model.cuda() 
  
  optimizer= optim.SGD(model.parameters(), lr=_lr, momentum=0.9)
  val_acc_history = []
  
  best_model_wts = deepcopy(model.state_dict()) 
  best_acc = 0.0

  for epoch in tqdm(range(1, num_epochs+1)):
      
      print('Epoch {}/{}'.format(epoch, num_epochs))
      print('-' * 10)

      # Each epoch has a training and validation phase
      for phase in ['train', 'val']:
          print(f"Current mode: {phase}")
          if phase == 'train':
              model.train()  # Set model to training mode
          else:
              model.eval()  # Set model to evaluate mode

          running_loss = 0.0
          running_corrects = 0
        
          # Iterate over data.
          for i, data in enumerate(dataloaders[phase], 0):
              
              tt = train_transform[name] if 'train' in phase else val_transform[name]
              inputs = data[0] if 'audio' in name else data[1]
              
              samples = []
              if 'visual' in name:
                  for batch_sample in inputs:
                      batch = []
                      for window in batch_sample:
                          #current = np.moveaxis(np.array(window), -1, 0)
                          #print(current.shape)
                          sample = tt(image=(window.numpy()))["image"]
                          batch.append(sample)
                      
                      samples.append(torch.stack(batch))
              else: # audio
                  for batch_sample in inputs:
                      batch = []
                      for window in batch_sample:
                          sample = tt(image=window.numpy())["image"]
                          
                          batch.append(sample)
                      samples.append(torch.stack(batch))
              inputs = torch.stack(samples)
              inputs = inputs.to(device)
              labels = (data[-1].float()).to(device)
              
              
              # zero the parameter gradients
              optimizer.zero_grad()

              # forward
              # track history if only in train
              with torch.set_grad_enabled(phase == 'train'):
                  #print(f"Inputs size: {inputs.size()}")
                  outputs = model(inputs)
                  #print(f"Outputs size: {outputs.size()}")
                  loss = one_hot_ce_loss(outputs, labels)
                  #print(f"Loss size: {loss.size()}")
                  # backward + optimize only if in training phase
                  if phase == 'train':
                      loss.backward()
                      optimizer.step()
              # convert model output to one hot encoding
              _, preds = torch.max(outputs, 1)
              _, targets = torch.max(labels, dim=1)
              # statistics
              running_loss += loss.item() * labels.size(0)
              running_corrects += torch.sum(preds == targets)
          
          epoch_loss = running_loss / len(dataloaders[phase].dataset)
          epoch_acc = 100 * (running_corrects.double() / len(dataloaders[phase].dataset))
          
          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))
          print()

          # deep copy the model
          if phase == 'val' and epoch_acc > best_acc:
              best_acc = epoch_acc
              best_model_wts = deepcopy(model.state_dict())
          if phase == 'val':
              val_acc_history.append(epoch_acc.float())

          if phase == 'train' and (epoch % 50 == 0 or epoch == 1):
              path = datapath + "Models/" + name + "/"
              filename = name + '_' + phase + '_' + "epoch_" + str(epoch) + '.pth'
              print(f"Storing {filename}")
              with open(path + filename, 'w'):
                  pass
              torch.save(model.state_dict(), path + filename)

  time_elapsed = time.time() - since
  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
  print('Best val Acc: {:4f}'.format(best_acc))

  # load best model weights
  model.load_state_dict(best_model_wts)
  path = datapath + "Models/" + name + "/"
  filepath = datapath + "Models/" + name + '_' + "model.pth"
  with(open(filepath, 'w')):
      pass
  torch.save(model.state_dict(), filepath)

  with open(path + 'overall_acc.txt', 'w') as f:
      for item in val_acc_history:
          f.write("%s\n" % item)

  return model, val_acc_history
